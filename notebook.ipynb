{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# SageMaker Studio Notebook for Data Transformation, Training, Model Registeration and Deployment\n",
    "\n",
    "## Transform the data and train a model inside a Jupyter notebook.\n",
    "\n",
    "In this workshop we will demonstrate traditional approach to model development and training directly in parameterized Jupyter notebooks.\n",
    "\n",
    "In this first notebook we will predict house prices based on the well-known [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) with a simple regression model in Tensorflow 2. This public dataset contains 13 features regarding housing stock of towns in the Boston area.  Features include average number of rooms, accessibility to radial highways, adjacency to a major river, etc.  \n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for training and test data.  We'll also set up a SageMaker Session to perform various operations, and specify an Amazon S3 bucket to hold input data and output.  The default bucket used here is created by SageMaker if it doesn't already exist, and named in accordance with the AWS account ID and AWS Region.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "batch_dir = os.path.join(os.getcwd(), 'data/batch')\n",
    "os.makedirs(batch_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Dataset transformation <a class=\"anchor\" id=\"SageMakerProcessing\">\n",
    "\n",
    "Next, we'll transform the dataset. In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances. \n",
    "\n",
    "We'll now save the raw feature data, and also save the labels for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(raw_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(raw_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next, we'll execute the data preprocessing as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = np.load(os.path.join(raw_dir, 'x_train.npy'))\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_files = glob.glob('{}/raw/*.npy'.format(data_dir))\n",
    "print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "for file in input_files:\n",
    "    raw = np.load(file)\n",
    "    # only transform feature columns\n",
    "    if 'y_' not in file:\n",
    "        transformed = scaler.transform(raw)\n",
    "    if 'train' in file:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(train_dir, 'y_train.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(train_dir, 'x_train.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "    else:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(test_dir, 'y_test.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TEST DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(test_dir, 'x_test.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#  Training <a class=\"anchor\" id=\"SageMakerHostedTraining\">\n",
    "\n",
    "Now that we've prepared a dataset, we can move on to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print('x train', x_train.shape,'y train', y_train.shape)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print('x test', x_test.shape,'y test', y_test.shape)\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(13,))\n",
    "    hidden_1 = tf.keras.layers.Dense(13, activation='tanh')(inputs)\n",
    "    hidden_2 = tf.keras.layers.Dense(6, activation='sigmoid')(hidden_1)\n",
    "    outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In Below cell values of 'batch_size', 'epochs' and 'Learning_rate' are provided at runtime in the notebook job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#parameterized cell\n",
    "x_train, y_train = get_train_data(train_dir)\n",
    "x_test, y_test = get_test_data(test_dir)\n",
    "\n",
    "device = '/cpu:0'\n",
    "print(device)\n",
    "#batch_size = 128\n",
    "#epochs = 80\n",
    "#learning_rate = 0.01\n",
    "print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "\n",
    "with tf.device(device):\n",
    "    model = get_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The unzipped archive should include the assets required by TensorFlow Serving to load the model and serve it, including a .pb file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.save('model' + '/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Scoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('model/1')\n",
    "\n",
    "x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"\\nTest MSE :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
